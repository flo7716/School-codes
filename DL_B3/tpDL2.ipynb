{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ef4606",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tpdeeplearning2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb376985",
   "metadata": {},
   "source": [
    "# Travaux pratiques - Perceptron multi-couche\n",
    "\n",
    "L'objectif de cette s√©ance de travaux pratiques est de faire √©voluer le mod√®le de r√©gression logistique utilis√© dans le pr√©c√©dent TP vers un r√©seau de neurones plus sophistiqu√©. Dans le TP pr√©c√©dent, nous avons travaill√© avec la r√©gression logistique, un mod√®le lin√©aire simple appartenant √† la famille des r√©seaux de neurones.\n",
    "\n",
    "Dans ce TP, nous allons nous attaquer √† l'impl√©mentation d'un perceptron multicouche (MLP - Multi-Layer Perceptron). Contrairement √† la r√©gression logistique, qui se limite √† des s√©parations lin√©aires, le perceptron multicouche a la capacit√© d'apprendre des fronti√®res de d√©cision non lin√©aires. De plus, les perceptrons sont consid√©r√©s comme des approximants universels pour les fonctions continues, ce qui signifie qu'ils sont extr√™mement puissants et forment la base de l'apprentissage profond.\n",
    "\n",
    "Nous commencerons par √©tudier l'impl√©mentation de la phase de propagation avant (forward pass) pour effectuer des pr√©dictions, puis la phase de r√©tropropagation (backward pass) pour entra√Æner un perceptron √† une couche cach√©e.\n",
    "\n",
    "Passons maintenant √† la recharge du jeu de donn√©es MNIST :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b8c7948e-96bc-4c28-b490-a195f7ea1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "473dbfe3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de MNIST depuis Keras\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# Transformation des images 28x28 en vecteur de dimension 784\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')\n",
    "# Normalisation entre 0 et 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d875844",
   "metadata": {},
   "source": [
    "## Pr√©diction avec un perceptron (*forward*)\n",
    "\n",
    "L‚Äôarchitecture du perceptron √† une seule couche cach√©e est illustr√©e dans la\n",
    "figure ci-dessous.\n",
    "\n",
    "Comme la derni√®re fois, on consid√®re les donn√©es de la base MNIST. Chaque image est\n",
    "repr√©sent√©e par un vecteur de taille $ 28^2=784 $. Le perceptron applique\n",
    "diff√©rentes op√©rations math√©matiques pour transformer l‚Äôentr√©e et produire\n",
    "la pr√©diction finale, c‚Äôest-√†-dire la cat√©gorie de l‚Äôimage :\n",
    "\n",
    "1. Une projection lin√©aire, qui va projeter chaque image sur un vecteur de dimensions $ (1, L) $. $ L $ repr√©sente ici la largeur (le nombre de neurones) de la couche cach√©e du perceptron, par exemple $ L=100 $.\n",
    "  En consid√©rant que chaque exemple $ \\mathbf{x_i} $ est un vecteur ligne $ (1,784) $, la projection lin√©aire est repr√©sent√©e par une matrice $ \\mathbf{W^h} $ $ (784, L) $ et un vecteur de biais $ \\mathbf{b^h} $\n",
    "  $ (1, L) $. La projection s‚Äô√©crit:  \n",
    "  $$\n",
    "  \\mathbf{u_i} = \\mathbf{x_i} \\mathbf{W^h} + \\mathbf{b^h}.\n",
    "  $$\n",
    "1. L‚Äôapplication d‚Äôune fonction de transfert non-lin√©aire, par exemple une sigmo√Øde :  \n",
    "  $$\n",
    "  \\forall j \\in \\left\\lbrace 1; L \\right\\rbrace, ~ h_{i,j} = \\frac{1}{1+\\exp(-u_{i,j})}.\n",
    "  $$\n",
    "1. Une deuxi√®me projection lin√©aire, qui va projeter la repr√©sentation interne (les activations de la couche cach√©e) de dimensions $ (1,L) $ en un vecteur de $ (1, K) $, avec $ K $ le nombre de classes consid√©r√©es (ici, 10). $ K $ repr√©sente le nombre de neurones en sortie, c‚Äôest-√†-dire la dimensionalit√© du vecteur pr√©dit. Cette op√©ration de projection lin√©aire est repr√©sent√©e par la matrice $ \\mathbf{W^y} $ de dimensions $ (L, K) $ et le vecteur de biais $ \\mathbf{b^y} $ de dimensions $ (1, K) $). Matriciellement, la projection est repr√©sent√©e par l‚Äôop√©ration :  \n",
    "  $$\n",
    "  \\mathbf{v_i} =\\mathbf{h_i} \\mathbf{W^y} + \\mathbf{b^y}.\n",
    "  $$\n",
    "1. Enfin, l‚Äôapplication d‚Äôune non-lin√©arit√© *softmax*. Comme pour la r√©gression logistique, cela permet de transformer les activations de sortie en probabilit√©s pour une distribution cat√©gorielle :  \n",
    "  $$\n",
    "  \\forall j \\in \\left\\lbrace 1; K \\right\\rbrace ~ y_{i,j} = \\frac{\\exp(v_{i,j})}{\\sum\\limits_{k=1}^K \\exp(v_{i,k})}.\n",
    "  $$\n",
    "\n",
    "\n",
    "Notre objectif pour cette s√©ance va √™tre d‚Äôimpl√©menter un perceptron (et son apprentissage) sur la base MNIST. Commen√ßons par transformer les √©tiquettes en vecteur encod√© au format *one-hot*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "85caf451",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "n_classes = 10\n",
    "# Conversion des √©tiquettes au format one-hot\n",
    "Y_train = to_categorical(y_train,n_classes)\n",
    "Y_test = to_categorical(y_test, n_classes)\n",
    "\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92293f",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "En reprenant le squelette du code de la r√©gression logistique, compl√©ter le code ci-dessous pour impl√©menter la *forward pass* (phase de pr√©diction) du perceptron multi-couche.\n",
    "\n",
    "Vous devrez notamment √©crire une fonction `forward(batch, Wh, bh, Wy, by)` qui renvoie la pr√©diction $ \\hat{\\mathbf{y}} $ ainsi que la matrice des activations de la couche cach√©e.\n",
    "Si l‚Äôon consid√®re un batch des donn√©es de taille $ n \\times 784 $, les param√®tres $ \\mathbf{W^h} $ ($ 784\\times L $), $ \\mathbf{b^h} $ ($ 1\\times L $),\n",
    "$ \\mathbf{W^y} $($ L\\times K $) et $ \\mathbf{b^y} $ ($ 1\\times K $), la fonction `forward` renvoie :\n",
    "\n",
    "- la pr√©diction $ \\mathbf{\\hat{Y}} $ sur le batch ($ n\\times K $),  \n",
    "- la matrice $ \\mathbf{H} $ des activations de la couche cach√©e ($ n\\times L $),  \n",
    "\n",
    "\n",
    "pour un batch de $ n $ exemples.\n",
    "\n",
    "#### üìå Formules √† implementer\n",
    "\n",
    "| **√âtape** | **Formule** |\n",
    "|-----------|------------|\n",
    "| **Couche cach√©e (sigmo√Øde)** | $$ H = \\sigma(X W_h + b_h) $$ avec $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$ |\n",
    "| **Sortie (softmax)** | $$ Y_{\\text{pred}} = \\text{softmax}(H W_y + b_y) $$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cac332ce",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Fonction d'activation sigmo√Øde \"\"\"\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid\n",
    "\n",
    "def forward(batch, Wh, bh, Wy, by):\n",
    "    \"\"\" \n",
    "    Propagation avant avec activation sigmo√Øde dans la couche cach√©e\n",
    "\n",
    "    Entr√©es :\n",
    "    - batch: un batch de n images de MNIST (n, 784)\n",
    "    - Wh: matrice des poids entr√©e -> couche cach√©e (784, hidden_size)\n",
    "    - bh: biais de la couche cach√©e (1, hidden_size)\n",
    "    - Wy: matrice des poids couche cach√©e -> sortie (hidden_size, 10)\n",
    "    - by: biais de la sortie (1, 10)\n",
    "\n",
    "    Renvoie :\n",
    "    - Y_pred: pr√©dictions de sortie (n, 10)\n",
    "    - H: activations de la couche cach√©e (n, hidden_size)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- PROPAGATION AVANT ----\n",
    "    # 1Ô∏è‚É£ Calcul de l'activation de la couche cach√©e\n",
    "    H = np.matmul(batch, Wh) + bh # Produit matriciel + biais\n",
    "    H = sigmoid(H) * H  # Application de la sigmo√Øde\n",
    "\n",
    "    # 2Ô∏è‚É£ Calcul de l'activation de la sortie\n",
    "    logits = np.matmul(H, Wy) + by  # Produit matriciel + biais\n",
    "    exp_logits = np.exp(logits - np.max(logits))  # appliquer la stabilit√© num√©rique\n",
    "    Y_pred = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # Softmax\n",
    "\n",
    "    return Y_pred, H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "387f8736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X_train, Wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530d300",
   "metadata": {},
   "source": [
    "## Apprentissage du perceptron (*backward*)\n",
    "\n",
    "Comme pour la r√©gression logistique, nous allons entra√Æner le perceptron √† l‚Äôaide de l‚Äôalgorithme\n",
    "de descente de gradient. Pour calculer les gradients par rapport aux param√®tres de la couche cach√©e,\n",
    "nous allons avoir besoin d‚Äôutiliser l‚Äôalgorithme de r√©tro-propagation du gradient (*backpropagation*).\n",
    "Rappellons que pour chaque batch d‚Äôexemples, l‚Äôalgorithme effectue une passe `forward` (impl√©ment√©e ci-dessus)\n",
    "qui permet de calculer la pr√©diction du perceptron pour les exemples du batch.\n",
    "\n",
    "La fonction de co√ªt consid√©r√©e sera encore l‚Äôentropie crois√©e entre la sortie pr√©dite et les √©tiquettes de\n",
    "supervision. On calculera alors le gradient de l‚Äôerreur par rapport √† tous les param√®tres du mod√®le,\n",
    "c‚Äôest-√†-dire:\n",
    "\n",
    "- $ \\mathbf{W^y} $ (dimensions $ (L, K) $),  \n",
    "- $ \\mathbf{b^y} $ (dimensions $ (1, K) $),  \n",
    "- $ \\mathbf{W^h} $ (dimensions $ (784, L) $),  \n",
    "- $ \\mathbf{b^h} $ (dimensions $ (1, L) $).  \n",
    "\n",
    "\n",
    "On rappelle ci-dessous les √©quations des gradients, effectu√©es depuis la sortie\n",
    "vers l‚Äôentr√©e du r√©seau :\n",
    "\n",
    "### Etape 1\n",
    "\n",
    "1. Mise √† jour de $ \\mathbf{W^y} $ et $ \\mathbf{b^y} $:  \n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v_i}} = \\mathbf{\\delta^y_i} = \\mathbf{\\hat{y_i}} - \\mathbf{y_i^*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W^y}} = \\frac{1}{n} \\mathbf{H}^T (\\mathbf{\\hat{Y}} - \\mathbf{Y^*}) = \\frac{1}{n} \\mathbf{H}^T \\mathbf{\\Delta^y} \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b^y}} = \\frac{1}{n}\\sum_{i=1}^{n}(\\mathbf{\\hat{y_i}} - \\mathbf{y_i^*}) \\tag{2}\n",
    "$$\n",
    "\n",
    "o√π $ \\mathbf{H} $ est la matrice des couches cach√©es sur le batch\n",
    "($ 784 \\times L $), $ \\mathbf{\\hat{Y}} $ est la matrice\n",
    "des pr√©dictions sur le batch (taille $ n \\times K $),\n",
    "$ \\mathbf{Y^*} $ est la matrice des √©tiquettes\n",
    "issues de la supervision (*ground truth*, $ n \\times K $) et\n",
    "$ \\mathbf{\\Delta^y}=\\mathbf{\\hat{Y}}-\\mathbf{Y^*} $.\n",
    "\n",
    "1. Mise √† jour de $ \\mathbf{W^h} $et $ \\mathbf{b^h} $:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a9f036",
   "metadata": {},
   "source": [
    "### Etape 2\n",
    "\n",
    "Les gradients de $ \\mathcal{L} $ par rapport √† $ \\mathbf{W^h} $ et $ \\mathbf{b^h} $ s‚Äô√©crivent matriciellement sous la forme:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u_i}} = \\mathbf{\\delta^h_i} = \\mathbf{\\delta^y_i} \\mathbf{W^{y}}^T   \\odot \\sigma^{'}(\\mathbf{u_i}) = \\mathbf{\\delta^y_i} \\mathbf{W^{y}}^T \\odot \\mathbf{h_i} \\odot (1-\\mathbf{h_i})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W^h}} =  \\frac{1}{n} \\mathbf{X}^T \\mathbf{\\Delta^h}\n",
    "~~~\\text{et}~~~\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b^h}} = \\frac{1}{n}\\sum_{i=1}^{n}(\\delta^h_i)\n",
    "$$\n",
    "\n",
    "o√π $ \\mathbf{X} $ est la matrice des donn√©es sur le batch ($ n \\times 784 $) et $ \\mathbf{\\Delta^h} $ est la matrice des $ \\delta^h_i $ sur le batch ($ n \\times L $)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5792e52",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Compl√©ter la fonction `backward` ci-dessous qui calcule et renvoie les gradients de l‚Äôerreur par rapport aux param√®tres du perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6a90cd86",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def backward(Y_pred, Y, X, H, Wy):\n",
    "    \"\"\" Entr√©es:\n",
    "    - Y_pred: batch de vecteur des pr√©dictions (one-hot)\n",
    "    - Y: batch de vecteur des √©tiquettes (one-hot)\n",
    "    - X: batch d'images (au format vectoriel (n, 784))\n",
    "    - H: matrice des activations cach√©es\n",
    "    - Wy: matrice des poids\n",
    "\n",
    "    Renvoie:\n",
    "    - gradWy: gradient de l'erreur (entropie crois√©e) par rapport √† Wy\n",
    "    - gradby: gradient de l'erreur (entropie crois√©e) par rapport √† by\n",
    "    - gradWh: gradient de l'erreur (entropie crois√©e) par rapport √† Wh\n",
    "    - gradbh: gradient de l'erreur (entropie crois√©e) par rapport √† bh\n",
    "\n",
    "    \"\"\"\n",
    "    delta_y = Y_pred - Y\n",
    "    # Gradient pour la couche de sortie (identique √† la r√©gression logistique)\n",
    "    gradWy = np.matmul(H.T, delta_y) / Y.shape[0]\n",
    "    gradby = np.sum(delta_y, axis=0) / Y.shape[0]\n",
    "    # Gradient pour la couche cach√©e\n",
    "    delta_h = np.matmul(delta_y, Wy.T) * (H > 0)\n",
    "    gradWh = np.matmul(X.T, delta_h) / Y.shape[0]\n",
    "    gradbh = np.sum(delta_h, axis=0) / Y.shape[0]\n",
    "    \n",
    "    return gradWy, gradby, gradWh, gradbh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4eae09",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "La fonction de co√ªt de l‚ÄôEq. [(3)](tpDeepLearning1.ipynb#equation-ce) est-elle convexe par\n",
    "rapport aux param√®tres $ \\mathbf{W} $, $ \\mathbf{b} $ du mod√®le ? Avec un pas de gradient bien choisi, peut-on assurer la\n",
    "convergence vers le minimum global de la solution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae44de0a",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Compl√©ter le code ci-dessous de sorte √† appliquer la descente de gradient sur le perceptron multi-couche d√©fini par les param√®tres `Wy, Wh, by, bh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a03f1082",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch : 0\n",
      "Epoch 0/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 1\n",
      "Epoch 1/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 2\n",
      "Epoch 2/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 3\n",
      "Epoch 3/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 4\n",
      "Epoch 4/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 5\n",
      "Epoch 5/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 6\n",
      "Epoch 6/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 7\n",
      "Epoch 7/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 8\n",
      "Epoch 8/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 9\n",
      "Epoch 9/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 10\n",
      "Epoch 10/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 11\n",
      "Epoch 11/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 12\n",
      "Epoch 12/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 13\n",
      "Epoch 13/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 14\n",
      "Epoch 14/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 15\n",
      "Epoch 15/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 16\n",
      "Epoch 16/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 17\n",
      "Epoch 17/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 18\n",
      "Epoch 18/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 19\n",
      "Epoch 19/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 20\n",
      "Epoch 20/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 21\n",
      "Epoch 21/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 22\n",
      "Epoch 22/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 23\n",
      "Epoch 23/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 24\n",
      "Epoch 24/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 25\n",
      "Epoch 25/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 26\n",
      "Epoch 26/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 27\n",
      "Epoch 27/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 28\n",
      "Epoch 28/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n",
      "Processing epoch : 29\n",
      "Epoch 29/30, accuracy (train) = 11.24, accuracy (test) = 11.35\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "N, d = X_train.shape # N exemples, dimension d\n",
    "hidden_size = 100 # Nombre de neurones de la couche cach√©e\n",
    "# Initialisation des poids et des biais\n",
    "Wy = np.zeros((hidden_size, n_classes))\n",
    "Wh = np.zeros((d, hidden_size))\n",
    "by = np.zeros((1, n_classes))\n",
    "bh = np.zeros((1, hidden_size))\n",
    "\n",
    "n_epochs = 30 # Nombre d'epochs de la descente de gradient\n",
    "eta = 1e-1 # Learning rate (pas d'apprentissage)\n",
    "batch_size = 128 # Taille du lot\n",
    "n_batches = int(float(N) / batch_size)\n",
    "\n",
    "# Allocation des matrices pour stocker les valeurs des gradients\n",
    "gradWy = np.zeros((hidden_size, n_classes))\n",
    "gradWh = np.zeros((d, hidden_size))\n",
    "gradby = np.zeros((1, n_classes))\n",
    "gradbh = np.zeros((1, hidden_size))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Processing epoch : {epoch}\")\n",
    "    for batch_idx in range(n_batches):\n",
    "        # ********* √Ä compl√©ter **********              \n",
    "        X = X_train[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        Y = Y_train[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        # FORWARD PASS : calculer la pr√©diction y √† partir des param√®tres courants pour les images du batch\n",
    "        Y_pred, H = forward(X, Wh, bh, Wy, by)\n",
    "        # BACKWARD PASS :\n",
    "        # 1) calculer les gradients de l'erreur par rapport √† W et b\n",
    "        gradWy, gradby, gradWh, gradbh = backward(Y_pred, Y, X, H, Wy)\n",
    "        # 2) mettre √† jour les param√®tres W et b selon la descente de gradient\n",
    "        Wy = Wy - eta * gradWy\n",
    "        by = by - eta * gradby\n",
    "        Wh = Wh - eta * gradWh\n",
    "        bh = bh - eta * gradbh\n",
    "        \n",
    "    \n",
    "    pred_train = forward(X_train, Wh, bh, Wy, by)[0].argmax(axis=1)\n",
    "    pred_test = forward(X_test, Wh, bh, Wy, by)[0].argmax(axis=1)\n",
    "        \n",
    "\n",
    "    print(f\"Epoch {epoch}/{n_epochs}, accuracy (train) = {accuracy_score(pred_train,Y_train.argmax(axis=1))*100:.2f}, accuracy (test) = {accuracy_score(pred_test, Y_test.argmax(axis=1))*100:.2f}\")\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d90f8",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Tester deux autres initialisations :\n",
    "\n",
    "- initialiser les poids avec une loi normale de moyenne nulle et d‚Äô√©cart type √† d√©terminer, par exemple $ 10^{-1} $,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5f608-5f86-4cb6-bc99-56b00fd6419d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cd8f7c4-0cd0-482a-8b63-c149abfc9825",
   "metadata": {},
   "source": [
    "##### Utilise le package sklearn pour entra√Æner un MLP (avec la m√™me architecture que le r√©seau pr√©c√©dent). √âvalue ensuite le mod√®le et compare les r√©sultats obtenus avec ceux du mod√®le pr√©c√©dent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb3533-eb89-4842-944e-eeb513fb4dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "date": 1725613532.7602315,
  "filename": "tpDeepLearning2.rst",
  "kernelspec": {
   "display_name": "jpn_florian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "title": "Travaux pratiques - Perceptron multi-couche"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
