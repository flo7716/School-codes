{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ef4606",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tpdeeplearning2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb376985",
   "metadata": {},
   "source": [
    "# Travaux pratiques - Perceptron multi-couche\n",
    "\n",
    "L'objectif de cette s√©ance de travaux pratiques est de faire √©voluer le mod√®le de r√©gression logistique utilis√© dans le pr√©c√©dent TP vers un r√©seau de neurones plus sophistiqu√©. Dans le TP pr√©c√©dent, nous avons travaill√© avec la r√©gression logistique, un mod√®le lin√©aire simple appartenant √† la famille des r√©seaux de neurones.\n",
    "\n",
    "Dans ce TP, nous allons nous attaquer √† l'impl√©mentation d'un perceptron multicouche (MLP - Multi-Layer Perceptron). Contrairement √† la r√©gression logistique, qui se limite √† des s√©parations lin√©aires, le perceptron multicouche a la capacit√© d'apprendre des fronti√®res de d√©cision non lin√©aires. De plus, les perceptrons sont consid√©r√©s comme des approximants universels pour les fonctions continues, ce qui signifie qu'ils sont extr√™mement puissants et forment la base de l'apprentissage profond.\n",
    "\n",
    "Nous commencerons par √©tudier l'impl√©mentation de la phase de propagation avant (forward pass) pour effectuer des pr√©dictions, puis la phase de r√©tropropagation (backward pass) pour entra√Æner un perceptron √† une couche cach√©e.\n",
    "\n",
    "Passons maintenant √† la recharge du jeu de donn√©es MNIST :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b8c7948e-96bc-4c28-b490-a195f7ea1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "473dbfe3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de MNIST depuis Keras\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# Transformation des images 28x28 en vecteur de dimension 784\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')\n",
    "# Normalisation entre 0 et 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d875844",
   "metadata": {},
   "source": [
    "## Pr√©diction avec un perceptron (*forward*)\n",
    "\n",
    "L‚Äôarchitecture du perceptron √† une seule couche cach√©e est illustr√©e dans la\n",
    "figure ci-dessous.\n",
    "\n",
    "Comme la derni√®re fois, on consid√®re les donn√©es de la base MNIST. Chaque image est\n",
    "repr√©sent√©e par un vecteur de taille $ 28^2=784 $. Le perceptron applique\n",
    "diff√©rentes op√©rations math√©matiques pour transformer l‚Äôentr√©e et produire\n",
    "la pr√©diction finale, c‚Äôest-√†-dire la cat√©gorie de l‚Äôimage :\n",
    "\n",
    "1. Une projection lin√©aire, qui va projeter chaque image sur un vecteur de dimensions $ (1, L) $. $ L $ repr√©sente ici la largeur (le nombre de neurones) de la couche cach√©e du perceptron, par exemple $ L=100 $.\n",
    "  En consid√©rant que chaque exemple $ \\mathbf{x_i} $ est un vecteur ligne $ (1,784) $, la projection lin√©aire est repr√©sent√©e par une matrice $ \\mathbf{W^h} $ $ (784, L) $ et un vecteur de biais $ \\mathbf{b^h} $\n",
    "  $ (1, L) $. La projection s‚Äô√©crit:  \n",
    "  $$\n",
    "  \\mathbf{u_i} = \\mathbf{x_i} \\mathbf{W^h} + \\mathbf{b^h}.\n",
    "  $$\n",
    "1. L‚Äôapplication d‚Äôune fonction de transfert non-lin√©aire, par exemple une sigmo√Øde :  \n",
    "  $$\n",
    "  \\forall j \\in \\left\\lbrace 1; L \\right\\rbrace, ~ h_{i,j} = \\frac{1}{1+\\exp(-u_{i,j})}.\n",
    "  $$\n",
    "1. Une deuxi√®me projection lin√©aire, qui va projeter la repr√©sentation interne (les activations de la couche cach√©e) de dimensions $ (1,L) $ en un vecteur de $ (1, K) $, avec $ K $ le nombre de classes consid√©r√©es (ici, 10). $ K $ repr√©sente le nombre de neurones en sortie, c‚Äôest-√†-dire la dimensionalit√© du vecteur pr√©dit. Cette op√©ration de projection lin√©aire est repr√©sent√©e par la matrice $ \\mathbf{W^y} $ de dimensions $ (L, K) $ et le vecteur de biais $ \\mathbf{b^y} $ de dimensions $ (1, K) $). Matriciellement, la projection est repr√©sent√©e par l‚Äôop√©ration :  \n",
    "  $$\n",
    "  \\mathbf{v_i} =\\mathbf{h_i} \\mathbf{W^y} + \\mathbf{b^y}.\n",
    "  $$\n",
    "1. Enfin, l‚Äôapplication d‚Äôune non-lin√©arit√© *softmax*. Comme pour la r√©gression logistique, cela permet de transformer les activations de sortie en probabilit√©s pour une distribution cat√©gorielle :  \n",
    "  $$\n",
    "  \\forall j \\in \\left\\lbrace 1; K \\right\\rbrace ~ y_{i,j} = \\frac{\\exp(v_{i,j})}{\\sum\\limits_{k=1}^K \\exp(v_{i,k})}.\n",
    "  $$\n",
    "\n",
    "\n",
    "Notre objectif pour cette s√©ance va √™tre d‚Äôimpl√©menter un perceptron (et son apprentissage) sur la base MNIST. Commen√ßons par transformer les √©tiquettes en vecteur encod√© au format *one-hot*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "85caf451",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "n_classes = 10\n",
    "# Conversion des √©tiquettes au format one-hot\n",
    "Y_train = to_categorical(y_train,n_classes)\n",
    "Y_test = to_categorical(y_test, n_classes)\n",
    "\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92293f",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "En reprenant le squelette du code de la r√©gression logistique, compl√©ter le code ci-dessous pour impl√©menter la *forward pass* (phase de pr√©diction) du perceptron multi-couche.\n",
    "\n",
    "Vous devrez notamment √©crire une fonction `forward(batch, Wh, bh, Wy, by)` qui renvoie la pr√©diction $ \\hat{\\mathbf{y}} $ ainsi que la matrice des activations de la couche cach√©e.\n",
    "Si l‚Äôon consid√®re un batch des donn√©es de taille $ n \\times 784 $, les param√®tres $ \\mathbf{W^h} $ ($ 784\\times L $), $ \\mathbf{b^h} $ ($ 1\\times L $),\n",
    "$ \\mathbf{W^y} $($ L\\times K $) et $ \\mathbf{b^y} $ ($ 1\\times K $), la fonction `forward` renvoie :\n",
    "\n",
    "- la pr√©diction $ \\mathbf{\\hat{Y}} $ sur le batch ($ n\\times K $),  \n",
    "- la matrice $ \\mathbf{H} $ des activations de la couche cach√©e ($ n\\times L $),  \n",
    "\n",
    "\n",
    "pour un batch de $ n $ exemples.\n",
    "\n",
    "#### üìå Formules √† implementer\n",
    "\n",
    "| **√âtape** | **Formule** |\n",
    "|-----------|------------|\n",
    "| **Couche cach√©e (sigmo√Øde)** | $$ H = \\sigma(X W_h + b_h) $$ avec $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$ |\n",
    "| **Sortie (softmax)** | $$ Y_{\\text{pred}} = \\text{softmax}(H W_y + b_y) $$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cac332ce",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Fonction d'activation sigmo√Øde \"\"\"\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid\n",
    "\n",
    "def forward(batch, Wh, bh, Wy, by):\n",
    "    \"\"\" \n",
    "    Propagation avant avec activation sigmo√Øde dans la couche cach√©e\n",
    "\n",
    "    Entr√©es :\n",
    "    - batch: un batch de n images de MNIST (n, 784)\n",
    "    - Wh: matrice des poids entr√©e -> couche cach√©e (784, hidden_size)\n",
    "    - bh: biais de la couche cach√©e (1, hidden_size)\n",
    "    - Wy: matrice des poids couche cach√©e -> sortie (hidden_size, 10)\n",
    "    - by: biais de la sortie (1, 10)\n",
    "\n",
    "    Renvoie :\n",
    "    - Y_pred: pr√©dictions de sortie (n, 10)\n",
    "    - H: activations de la couche cach√©e (n, hidden_size)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- PROPAGATION AVANT ----\n",
    "    # 1Ô∏è‚É£ Calcul de l'activation de la couche cach√©e\n",
    "    H = np.matmul(batch, Wh) + bh # Produit matriciel + biais\n",
    "    H = sigmoid(H) * H  # Application de la sigmo√Øde\n",
    "\n",
    "    # 2Ô∏è‚É£ Calcul de l'activation de la sortie\n",
    "    logits = np.matmul(H, Wy) + by  # Produit matriciel + biais\n",
    "    exp_logits = np.exp(logits - np.max(logits))  # appliquer la stabilit√© num√©rique\n",
    "    Y_pred = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # Softmax\n",
    "\n",
    "    return Y_pred, H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "387f8736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X_train, Wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530d300",
   "metadata": {},
   "source": [
    "## Apprentissage du perceptron (*backward*)\n",
    "\n",
    "Comme pour la r√©gression logistique, nous allons entra√Æner le perceptron √† l‚Äôaide de l‚Äôalgorithme\n",
    "de descente de gradient. Pour calculer les gradients par rapport aux param√®tres de la couche cach√©e,\n",
    "nous allons avoir besoin d‚Äôutiliser l‚Äôalgorithme de r√©tro-propagation du gradient (*backpropagation*).\n",
    "Rappellons que pour chaque batch d‚Äôexemples, l‚Äôalgorithme effectue une passe `forward` (impl√©ment√©e ci-dessus)\n",
    "qui permet de calculer la pr√©diction du perceptron pour les exemples du batch.\n",
    "\n",
    "La fonction de co√ªt consid√©r√©e sera encore l‚Äôentropie crois√©e entre la sortie pr√©dite et les √©tiquettes de\n",
    "supervision. On calculera alors le gradient de l‚Äôerreur par rapport √† tous les param√®tres du mod√®le,\n",
    "c‚Äôest-√†-dire:\n",
    "\n",
    "- $ \\mathbf{W^y} $ (dimensions $ (L, K) $),  \n",
    "- $ \\mathbf{b^y} $ (dimensions $ (1, K) $),  \n",
    "- $ \\mathbf{W^h} $ (dimensions $ (784, L) $),  \n",
    "- $ \\mathbf{b^h} $ (dimensions $ (1, L) $).  \n",
    "\n",
    "\n",
    "On rappelle ci-dessous les √©quations des gradients, effectu√©es depuis la sortie\n",
    "vers l‚Äôentr√©e du r√©seau :\n",
    "\n",
    "### Etape 1\n",
    "\n",
    "1. Mise √† jour de $ \\mathbf{W^y} $ et $ \\mathbf{b^y} $:  \n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v_i}} = \\mathbf{\\delta^y_i} = \\mathbf{\\hat{y_i}} - \\mathbf{y_i^*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W^y}} = \\frac{1}{n} \\mathbf{H}^T (\\mathbf{\\hat{Y}} - \\mathbf{Y^*}) = \\frac{1}{n} \\mathbf{H}^T \\mathbf{\\Delta^y} \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b^y}} = \\frac{1}{n}\\sum_{i=1}^{n}(\\mathbf{\\hat{y_i}} - \\mathbf{y_i^*}) \\tag{2}\n",
    "$$\n",
    "\n",
    "o√π $ \\mathbf{H} $ est la matrice des couches cach√©es sur le batch\n",
    "($ 784 \\times L $), $ \\mathbf{\\hat{Y}} $ est la matrice\n",
    "des pr√©dictions sur le batch (taille $ n \\times K $),\n",
    "$ \\mathbf{Y^*} $ est la matrice des √©tiquettes\n",
    "issues de la supervision (*ground truth*, $ n \\times K $) et\n",
    "$ \\mathbf{\\Delta^y}=\\mathbf{\\hat{Y}}-\\mathbf{Y^*} $.\n",
    "\n",
    "1. Mise √† jour de $ \\mathbf{W^h} $et $ \\mathbf{b^h} $:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a9f036",
   "metadata": {},
   "source": [
    "### Etape 2\n",
    "\n",
    "Les gradients de $ \\mathcal{L} $ par rapport √† $ \\mathbf{W^h} $ et $ \\mathbf{b^h} $ s‚Äô√©crivent matriciellement sous la forme:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u_i}} = \\mathbf{\\delta^h_i} = \\mathbf{\\delta^y_i} \\mathbf{W^{y}}^T   \\odot \\sigma^{'}(\\mathbf{u_i}) = \\mathbf{\\delta^y_i} \\mathbf{W^{y}}^T \\odot \\mathbf{h_i} \\odot (1-\\mathbf{h_i})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W^h}} =  \\frac{1}{n} \\mathbf{X}^T \\mathbf{\\Delta^h}\n",
    "~~~\\text{et}~~~\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b^h}} = \\frac{1}{n}\\sum_{i=1}^{n}(\\delta^h_i)\n",
    "$$\n",
    "\n",
    "o√π $ \\mathbf{X} $ est la matrice des donn√©es sur le batch ($ n \\times 784 $) et $ \\mathbf{\\Delta^h} $ est la matrice des $ \\delta^h_i $ sur le batch ($ n \\times L $)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5792e52",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Compl√©ter la fonction `backward` ci-dessous qui calcule et renvoie les gradients de l‚Äôerreur par rapport aux param√®tres du perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6a90cd86",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def backward(Y_pred, Y, X, H, Wy):\n",
    "    \"\"\" Entr√©es:\n",
    "    - Y_pred: batch de vecteur des pr√©dictions (one-hot)\n",
    "    - Y: batch de vecteur des √©tiquettes (one-hot)\n",
    "    - X: batch d'images (au format vectoriel (n, 784))\n",
    "    - H: matrice des activations cach√©es\n",
    "    - Wy: matrice des poids\n",
    "\n",
    "    Renvoie:\n",
    "    - gradWy: gradient de l'erreur (entropie crois√©e) par rapport √† Wy\n",
    "    - gradby: gradient de l'erreur (entropie crois√©e) par rapport √† by\n",
    "    - gradWh: gradient de l'erreur (entropie crois√©e) par rapport √† Wh\n",
    "    - gradbh: gradient de l'erreur (entropie crois√©e) par rapport √† bh\n",
    "\n",
    "    \"\"\"\n",
    "    delta_y = Y_pred - Y\n",
    "    # Gradient pour la couche de sortie (identique √† la r√©gression logistique)\n",
    "    gradWy = np.matmul(H.T, delta_y) / Y.shape[0]\n",
    "    gradby = np.sum(delta_y, axis=0) / Y.shape[0]\n",
    "    # Gradient pour la couche cach√©e\n",
    "    delta_h = np.matmul(delta_y, Wy.T) * (H > 0)\n",
    "    gradWh = np.matmul(X.T, delta_h) / Y.shape[0]\n",
    "    gradbh = np.sum(delta_h, axis=0) / Y.shape[0]\n",
    "    \n",
    "    return gradWy, gradby, gradWh, gradbh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4eae09",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "La fonction de co√ªt de l‚ÄôEq. [(3)](tpDeepLearning1.ipynb#equation-ce) est-elle convexe par\n",
    "rapport aux param√®tres $ \\mathbf{W} $, $ \\mathbf{b} $ du mod√®le ? Avec un pas de gradient bien choisi, peut-on assurer la\n",
    "convergence vers le minimum global de la solution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae44de0a",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Compl√©ter le code ci-dessous de sorte √† appliquer la descente de gradient sur le perceptron multi-couche d√©fini par les param√®tres `Wy, Wh, by, bh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f1082",
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch : 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [60000, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m pred_train \u001b[38;5;241m=\u001b[39m forward(X_train, Wh, bh, Wy, by)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m pred_test \u001b[38;5;241m=\u001b[39m forward(X_test, Wh, bh, Wy, by)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy (train) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(pred_train,Y_train\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy (test) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(pred_test,\u001b[38;5;250m \u001b[39mY_test\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/jpn_florian/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/jpn_florian/lib/python3.8/site-packages/sklearn/metrics/_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/jpn_florian/lib/python3.8/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/jpn_florian/lib/python3.8/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [60000, 10]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "N, d = X_train.shape # N exemples, dimension d\n",
    "hidden_size = 100 # Nombre de neurones de la couche cach√©e\n",
    "# Initialisation des poids et des biais\n",
    "Wy = np.zeros((hidden_size, n_classes))\n",
    "Wh = np.zeros((d, hidden_size))\n",
    "by = np.zeros((1, n_classes))\n",
    "bh = np.zeros((1, hidden_size))\n",
    "\n",
    "n_epochs = 30 # Nombre d'epochs de la descente de gradient\n",
    "eta = 1e-1 # Learning rate (pas d'apprentissage)\n",
    "batch_size = 128 # Taille du lot\n",
    "n_batches = int(float(N) / batch_size)\n",
    "\n",
    "# Allocation des matrices pour stocker les valeurs des gradients\n",
    "gradWy = np.zeros((hidden_size, n_classes))\n",
    "gradWh = np.zeros((d, hidden_size))\n",
    "gradby = np.zeros((1, n_classes))\n",
    "gradbh = np.zeros((1, hidden_size))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Processing epoch : {epoch}\")\n",
    "    for batch_idx in range(n_batches):\n",
    "        # ********* √Ä compl√©ter **********              \n",
    "        X = X_train[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        Y = Y_train[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        # FORWARD PASS : calculer la pr√©diction y √† partir des param√®tres courants pour les images du batch\n",
    "        Y_pred, H = forward(X, Wh, bh, Wy, by)\n",
    "        # BACKWARD PASS :\n",
    "        # 1) calculer les gradients de l'erreur par rapport √† W et b\n",
    "        gradWy, gradby, gradWh, gradbh = backward(Y_pred, Y, X, H, Wy)\n",
    "        # 2) mettre √† jour les param√®tres W et b selon la descente de gradient\n",
    "        Wy = Wy - eta * gradWy\n",
    "        by = by - eta * gradby\n",
    "        Wh = Wh - eta * gradWh\n",
    "        bh = bh - eta * gradbh\n",
    "        \n",
    "    \n",
    "    pred_train = forward(X_train, Wh, bh, Wy, by)[0].argmax(axis=1)\n",
    "    pred_test = forward(X_test, Wh, bh, Wy, by)[0].argmax(axis=1)\n",
    "        \n",
    "\n",
    "    print(f\"Epoch {epoch}/{n_epochs}, accuracy (train) = {accuracy_score(pred_train,Y_train.argmax(axis=1))*100:.2f}, accuracy (test) = {accuracy_score(pred_test, Y_test.argmax(axis=1))*100:.2f}\")\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d90f8",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Tester deux autres initialisations :\n",
    "\n",
    "- initialiser les poids avec une loi normale de moyenne nulle et d‚Äô√©cart type √† d√©terminer, par exemple $ 10^{-1} $,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5f608-5f86-4cb6-bc99-56b00fd6419d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cd8f7c4-0cd0-482a-8b63-c149abfc9825",
   "metadata": {},
   "source": [
    "##### Utilise le package sklearn pour entra√Æner un MLP (avec la m√™me architecture que le r√©seau pr√©c√©dent). √âvalue ensuite le mod√®le et compare les r√©sultats obtenus avec ceux du mod√®le pr√©c√©dent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb3533-eb89-4842-944e-eeb513fb4dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "date": 1725613532.7602315,
  "filename": "tpDeepLearning2.rst",
  "kernelspec": {
   "display_name": "jpn_florian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "title": "Travaux pratiques - Perceptron multi-couche"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
