{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP SVM Bachelor 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les données\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# afficher la description de la base de données\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iris.feature_names[]\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM à noyau linéaire\n",
    "\n",
    "Nous allons nous limiter pour l'instant à deux classes : setosa et virginica, et à deux features (pour visualiser) : sepal length and sepal width. Nous allons entrainer une SVM linéaire et afficher l'hyperplan séparateur (en 2D une droite donc).\n",
    "\n",
    "Nous allons utiliser la classe [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) du module svm de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = ###\n",
    "\n",
    "# on sélectionne les données voulues (2 classes et 2 features)\n",
    "X = ###\n",
    "print(X.shape)\n",
    "y = ###\n",
    "print(y.shape)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentons l'hyperplan séparateur !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# plot the point cloud\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)\n",
    "\n",
    "# get frame limits\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# visualize support vectors with a cross\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], \n",
    "           s=50, linewidth=1, marker='x', color='k')\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], \n",
    "           alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "# format the plot\n",
    "plt.xticks(fontweight=\"bold\", fontsize=15)\n",
    "plt.yticks(fontweight=\"bold\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Question :__ Où sont situés les vecteurs de support ?\n",
    "\n",
    "__Réponse :__ ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affichons la performance du prédicteur avec la methode score() et classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Quelle mesure de performance est calculée par `clf.score` ? Référez-vous à la [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.score). Que veut dire une performance de 1.0 ?\n",
    "\n",
    "__Réponse :__ ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons maintenant les deux classes Versicolour et Virginica !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ###\n",
    "\n",
    "# on sélectionne les données voulues (2 classes et 2 features)\n",
    "X = ###\n",
    "print(X.shape)\n",
    "y = ###\n",
    "print(y.shape)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# plot the point cloud\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Paired)\n",
    "\n",
    "# get frame limits\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# visualize support vectors with a cross\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], \n",
    "           s=50, linewidth=1, marker='x', color='k')\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], \n",
    "           alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "# format the plot\n",
    "plt.xticks(fontweight=\"bold\", fontsize=15)\n",
    "plt.yticks(fontweight=\"bold\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Où sont situés les vecteurs de support ?\n",
    "\n",
    "__Réponse :__ ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle est la performance de ce modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La performance est moins bonne que le premier exemple. Essayons d'utiliser un noyau pour avoir une séparation plus complexe et mieux coller aux données !\n",
    "\n",
    "### SVM à noyau non linéaire\n",
    "\n",
    "Nous allons utiliser un noyau RBF gaussien, pour plusieurs valeurs du paramètre gamma. En classe nous avons donné la formule \n",
    "\n",
    "$k(x, x') = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\frac{||x - x'||^2}{2 \\sigma^2}$ pour le noyau gaussien. \n",
    "\n",
    "__Question :__ À quoi correspond le paramètre gamma ?\n",
    "\n",
    "__Réponse :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genere plusieur valeurs de gamma (de 1 à 50)\n",
    "gamma_range = \n",
    "\n",
    "#tester le modèle pour chaque valeur de gamma\n",
    "for param in gamma_range:\n",
    "    clf = ###\n",
    "    clf.fit(X, y)\n",
    "    print(\"gamma: %.2f\" % param, \"score: %.2f\" % clf.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Affichez maintenant la frontière de séparation pour le dernier de ces classifieurs, commenter le résultat.\n",
    "\n",
    "__Réponse :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est-ce que ce modèle se __généralise__ bien, autrement dit, sera-t-il capable de faire de bonnes prédictions sur de nouvelles données que nous n'avons pas utilisées pour le construire ? \n",
    "\n",
    "Pour le savoir, nous allons séparer les données en un __jeu d'entraînement__ et un __jeu de test__. Nous allons entraîner nos SVMs sur le jeu d'entraînement seulement, et mesurer leur performance sur le jeu de test. Le jeu de test, étant inconnu au moment de l'entraînement, fait figure de nouvelles données. Pour cela nous allons utiliser la fonction `train_test_split` de scikit-learn, décrite [ici](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = ###\n",
    "\n",
    "#Initialisation des listes pour stocker les scores :\n",
    "acc_train, acc_test = list(), list()\n",
    "\n",
    "#Boucle sur les valeurs de gamma_range\n",
    "for param in gamma_range:\n",
    "    clf = ###\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Le score (précision, par exemple) du modèle est calculé sur les données d’entraînement et ajouté à la liste acc_train\n",
    "    ### CODE ###\n",
    "    # Le score du modèle est calculé sur les données de test et ajouté à la liste acc_test.\n",
    "    ### CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# plot train and test scores for different gamma values\n",
    "plt.plot(gamma_range, acc_train, label='train set', lw=4)\n",
    "plt.plot(gamma_range, acc_test, label='test set', lw=4)\n",
    "\n",
    "# add a legend\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "# format the plot\n",
    "plt.xlabel(\"Gamma\", fontweight=\"bold\", fontsize=20)\n",
    "plt.ylabel(\"Performance\", fontweight=\"bold\", fontsize=20)\n",
    "plt.xticks(fontweight=\"bold\", fontsize=15)\n",
    "plt.yticks(fontweight=\"bold\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Observez-vous un effet de surapprentissage ? Où ? \n",
    "\n",
    "__Réponse :__  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique ci-dessus nous donne envie de prendre pour le paramètre gamma une valeur proche de 10 ou 20. Mais attention ! Si nous prenons le paramètre qui marche le mieux sur le dataset de test, nous risquons aussi de surapprendre : nous aurons alors utilisé le jeu de test pour choisir le meilleur modèle, autrement dit, nous aurons touché aux données soi-disant inconnuees lors de l'apprentissage...\n",
    "\n",
    "Pour éviter cela, nous allons faire une __validation croisée__ (_cross-validation_) sur le jeu d'entraînement.\n",
    "\n",
    "Nous allons pour cela utiliser la classe [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) du module model_selection de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Définir les paramètres à tester\n",
    "parameters = {'kernel':####, \n",
    "              'C':####}\n",
    "\n",
    "# Initialiser un classifieur SVM\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Initialiser un gridsearch\n",
    "clf = ####\n",
    "\n",
    "# Faire tourner la validation croisée sur le jeu d'entraînement\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# format results from gridsearch\n",
    "scores = clf.cv_results_['mean_test_score'].reshape(len(parameters['kernel']), len(parameters['C']))\n",
    "\n",
    "# plot performance scores\n",
    "plt.imshow(scores, interpolation='none')\n",
    "#plt.imshow(scores, interpolation='none', cmap=\"RdBu_r\", vmin=0, vmax=1)\n",
    "\n",
    "# add a colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# format the plot\n",
    "plt.title(\"Score\", fontweight=\"bold\", fontsize=20)\n",
    "plt.xlabel(\"C\", fontweight=\"bold\", fontsize=18)\n",
    "plt.ylabel(\"Noyau\", fontweight=\"bold\", fontsize=18)\n",
    "plt.ylim((-0.5, 1.5))\n",
    "plt.xticks(np.arange(len(parameters['C'])), parameters['C'], fontsize=15)\n",
    "plt.yticks(np.arange(len(parameters['kernel'])), parameters['kernel'], rotation=90, fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Quel est le rôle du paramètre C ? Qu'observez-vous quand C est grand ? \n",
    "\n",
    "__Réponse :__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
