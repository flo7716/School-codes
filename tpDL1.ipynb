{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840f55de",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tpdeeplearning1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d6084",
   "metadata": {},
   "source": [
    "# Travaux pratiques - Premiers r√©seaux de neurones\n",
    "\n",
    "Au cours de cette s√©ance de travaux pratiques, vous allez √™tre amen√©s √† impl√©menter vous-m√™me l‚Äôapprentissage d‚Äôun r√©seau de neurones simple. Bien que de nombreuses biblioth√®ques existent pour automatiser cette t√¢che, il est tr√®s utile de se familiariser avec les concepts fondamentaux au moins une fois. Cela vous permettra d‚Äôavoir une meilleure compr√©hension des outils que nous utiliserons plus tard, comme Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79b434",
   "metadata": {},
   "source": [
    "## Jeu de donn√©es MNIST\n",
    "\n",
    "Lors de cette s√©ance, nous allons utiliser la base de donn√©es MNIST, compos√©e de 70 000 images de chiffres manuscrits en noir et blanc (60 000 pour l‚Äôentra√Ænement et 10 000 pour le test). L‚Äôobjectif est de d√©velopper un mod√®le capable d‚Äôidentifier automatiquement le chiffre √† partir de chaque image.\n",
    "\n",
    "Pour commencer, nous allons importer les donn√©es. √âtant donn√© qu‚Äôil s‚Äôagit d‚Äôun jeu de donn√©es largement utilis√© et standard, il est int√©gr√© dans plusieurs biblioth√®ques, comme Keras, ce qui nous permet de l‚Äôimporter facilement en une seule ligne de code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977c142-6bfc-4471-8e6b-481c2949a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow keras numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290b648-c045-4ca6-b8f7-df1acb7e5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import sys; print(sys.executable)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b4807a-9c84-4f8c-8165-2fa4110239d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show numpy\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3220bff",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "# Import de MNIST depuis Keras\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# Transformation des images 28x28 en vecteur de dimension 784\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')\n",
    "# Normalisation entre 0 et 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Affichage du nombre de'exemples\n",
    "print(f\"{X_train.shape[0]} exemples d'apprentissage\")\n",
    "print(f\"{X_test.shape[0]} exemples de test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebb0017",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Afficher √† l‚Äôaide de matplotlib les premi√®res images du jeu d‚Äôapprentissage. La fonction `plt.imshow()` (cf. [sa documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html)) peut vous √™tre utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7b9f9-9cdb-4ebe-85cf-84c1d78d16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "n_images = 10\n",
    "for i in range(n_images):\n",
    "    plt.subplot(1, n_images, i+1)\n",
    "    plt.imshow(###  COD  E###)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0f2f2",
   "metadata": {},
   "source": [
    "## Question :\n",
    "\n",
    "Quel est l‚Äôespace dans lequel se trouvent les images ? Quelle est sa dimension ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82065f2-be15-4ec6-b8cd-b8cb01ab5898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40c58781",
   "metadata": {},
   "source": [
    "## R√©gression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b020534b",
   "metadata": {},
   "source": [
    "### Mod√®le de pr√©diction\n",
    "\n",
    "Nous allons impl√©menter un mod√®le de classification lin√©aire simple : la r√©gression logistique. Concr√®tement, la r√©gression logistique est √©quivalente √† un r√©seau de neurones √† une seule couche. Il s‚Äôagit d‚Äôune projection du vecteur d‚Äôentr√©e $ \\mathbf{x_i} $ par un vecteur de param√®tres $ \\mathbf{w_{c}} $, plus un biais sclaaire $ b_c $, pour chaque classe.  Le sch√©ma ci-dessous illustre le mod√®le de r√©gression logistique avec un r√©seau de neurones.\n",
    "\n",
    "<img src=\"LR.png\" style=\"height:150px;\" align=\"center\">\n",
    "\n",
    "En l‚Äôoccurrence, pour MNIST $ \\mathbf{x}_i $ est de dimension 784 et il y a dix chiffres possibles, donc 10 classes diff√©rentes. Dans notre cas, on consid√®re que l‚Äôimage d‚Äôentr√©e est repr√©sent√©e sous sa forme ¬´ aplatie ¬ª, c‚Äôest-√†-dire un vecteur (1, 784).\n",
    "\n",
    "Pour simplifier les notations, on regroupe l‚Äôensemble des jeux de param√®tres $ \\mathbf{w_{c}} $ pour les 10 classes possibles dans une unique matrice $ \\mathbf{W} $ de dimensions $ 784\\times 10 $. De la m√™me fa√ßon, les biais sont regroup√©s dans un vecteur $ \\mathbf{b} $ de longueur 10. La sortie de la r√©gression logistique est un vecteur contenant une activation pour chaque classe, c‚Äôest-√†-dire $ \\mathbf{\\hat{s_i}} =\\mathbf{x_i}  \\mathbf{W}  + \\mathbf{b} $ de dimensions (1, 10).\n",
    "\n",
    "Afin de transformer les activations en de sortie en probabilit√©s pour une distribution cat√©gorielle, on ajoute une fonction d‚Äôactivation de *softmax* sur $ \\mathbf{\\hat{y_i}} = \\sigma(\\mathbf{s_i}) $. Cela nous permet d‚Äôobtenir en sortie un vecteur de pr√©dictions $ \\mathbf{\\hat{y_i}} $, de dimensions (1, 10),  qui repr√©sente la probabilit√© *a posteriori* $ p(\\mathbf{\\hat{y_i}} | \\mathbf{x_i}) $ pour chacune des 10 classes :\n",
    "\n",
    "\n",
    "<a id='equation-softmax'></a>\n",
    "$$\n",
    "p(\\hat{y}_{c,i} | \\mathbf{x_i}) ) = \\frac{e^{\\langle \\mathbf{x_i} ; \\mathbf{w_{c}}\\rangle + b_{c}}}{\\sum_{c'=1}^{10} e^{\\langle \\mathbf{x_i} ; \\mathbf{w_{c'}}\\rangle + b_{c'}}} \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15508582",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Quel est le nombre de param√®tres du mod√®le utilis√© ? Justifier le calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43ebb3-8de7-4183-875c-5e9223559cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16936d3e",
   "metadata": {},
   "source": [
    "### Formulation du probl√®me d‚Äôapprentissage\n",
    "\n",
    "Pour entra√Æner le r√©seau de neurones, c‚Äôest-√†-dire d√©terminer les valeurs optimales des param√®tres $ \\mathbf{W} $ et $ \\mathbf{b} $, on va comparer pour chaque exemple d‚Äôapprentissage la sortie pr√©dite $ \\mathbf{\\hat{y_i}} $ (√©quation [(1)](#equation-softmax)) pour l‚Äôimage $ \\mathbf{x_i} $ √† la sortie r√©elle $ \\mathbf{y_i^*} $ (v√©rit√© terrain issue de la supervision). Dans notre cas, on choisit d‚Äôencoder la cat√©gorie de l‚Äôimage $ \\mathbf{x_i} $ sous forme *one-hot*, c‚Äôest-√†-dire :\n",
    "\n",
    "\n",
    "<a id='equation-one-hot'></a>\n",
    "$$\n",
    "y_{c,i}^* =\n",
    " \\begin{cases}\n",
    "   1 & \\text{si c correspond √† l'indice de la classe de } \\mathbf{x_i}  \\\\\n",
    "   0 & \\text{sinon}\n",
    " \\end{cases} \\tag{2}\n",
    "$$\n",
    "\n",
    "G√©n√©rons les √©tiquettes (*labels*) au format *one-hot* ([(2)](#equation-one-hot)) √† l‚Äôaide de la fonction `to_categorical` (cf. [documentation de Keras](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b514892",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "n_classes = 10\n",
    "# Conversion des √©tiquettes (int) au format vectoriel one-hot\n",
    "Y_train = ...\n",
    "Y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5e911",
   "metadata": {},
   "source": [
    "L‚Äôerreur de pr√©diction sera d√©finie √† l‚Äôaide de l‚Äôentropie crois√©e (*cross-entropy*). Cette fonction de co√ªt s‚Äôapplique entre $ \\mathbf{\\hat{y_i}} $ et $ \\mathbf{y_i^*} $ par la formule:\n",
    "$ \\mathcal{L}(\\mathbf{\\hat{y_i}}, \\mathbf{y_i^*}) = -\\sum_{c=1}^{10} y_{c,i}^* \\log(\\hat{y}_{c,i}) = - \\log(\\hat{y}_{c^*,i}) $, o√π $ c^* $ correspond √† l‚Äôindice de la classe donn√©e par la supervision pour l‚Äôimage $ \\mathbf{x_i} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b5186",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "L‚Äôentropie crois√©e correspond en r√©alit√© √† la divergence de Kullback-Leiber pour des distributions cat√©gorielles. La divergence KL est une mesure de dissimilarit√© entre distributions de probabilit√©. Autrement dit, l‚Äôerreur que l‚Äôon mesure vise √† r√©duire l‚Äô√©cart entre la distribution r√©elle des cat√©gories et la distribution pr√©dite.\n",
    "\n",
    "La fonction de co√ªt finale correspond √† l‚Äôerreur d‚Äôapprentissage, c‚Äôest-√†-dire la moyenne l‚Äôentropie crois√©e sur l‚Äôensemble de la base d‚Äôapprentissage $ \\mathcal{D} $ constitu√©e des $ N=60000 $ images :\n",
    "\n",
    "\n",
    "<a id='equation-ce'></a>\n",
    "$$\n",
    "\\mathcal{L}_{\\mathbf{W},\\mathbf{b}}(\\mathcal{D})  = - \\frac{1}{N}\\sum_{i=1}^{N} \\log(\\hat{y}_{c^*,i}) \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c947826",
   "metadata": {},
   "source": [
    "### Optimisation du mod√®le\n",
    "\n",
    "Nous allons minimiser la fonction de co√ªt √† l‚Äôaide de l‚Äôalgorithme de descente de gradient appliqu√© sur les param√®tres $ \\mathbf{W} $ et $ \\mathbf{b} $ du mod√®le de r√©gression logistique. Pour ce faire, nous allons avoir besoin des gradients de l‚Äôentropie crois√©e par rapport √† $ \\mathbf{W} $ ainsi que $ \\mathbf{b} $. Nous pouvons nous appuyer sur la des d√©riv√©es cha√Æn√©es (*chain rule*, ou th√©or√®me de d√©rivation des fonctions compos√©es) :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} =  \\frac{1}{N}\\sum_{i=1}^{N} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y_i}}}  \\frac{\\partial \\mathbf{\\hat{y_i}}}{\\partial \\mathbf{s_i}} \\frac{\\partial \\mathbf{s_i}}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}} =  \\frac{1}{N}\\sum_{i=1}^{N} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y_i}}}  \\frac{\\partial \\mathbf{\\hat{y_i}}}{\\partial \\mathbf{s_i}} \\frac{\\partial \\mathbf{s_i}}{\\partial \\mathbf{b}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d7802",
   "metadata": {},
   "source": [
    "### Impl√©mentation de l‚Äôapprentissage\n",
    "\n",
    "Les gradients obtenus par les √©quations du gradients s‚Äô√©crivent sous forme ¬´ vectorielle ¬ª, ce qui rend les calculs efficaces avec des biblioth√®ques de calcul scientifique telles que `numpy`. Apr√®s calcul du gradient, les param√®tres sont mis √† jour de la fa√ßon suivante :\n",
    "\n",
    "\n",
    "<a id='equation-gradientupdatew'></a>\n",
    "$$\n",
    "\\mathbf{W}^{(t+1)} = \\mathbf{W}^{(t)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} \\tag{6}\n",
    "$$\n",
    "\n",
    "\n",
    "<a id='equation-gradientupdateb'></a>\n",
    "$$\n",
    "\\mathbf{b}^{(t+1)} = \\mathbf{b}^{(t)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}} \\tag{7}\n",
    "$$\n",
    "\n",
    "o√π $ \\eta $ est le pas de gradient (*learning rate*).\n",
    "\n",
    "En th√©orie, la descente de gradient n√©cessite de calculer les gradients de la fonction de co√ªt sur tout le jeu de donn√©es d‚Äôapprentissage. Toutefois, ce jeu de donn√©es est assez grand et les gradients peuvent √™tre longs √† calculer. En pratique, on impl√©mente plut√¥t une descente de gradient *stochastique*, c‚Äôest √† dire que les gradients aux √©quations [(4)](#equation-gradientw) et [(5)](#equation-gradientb) ne seront pas calcul√©s sur l‚Äôensemble des $ N=60000 $ images d‚Äôapprentissage, mais sur un sous-ensemble de $ n $ images appel√© *batch* ou *lot*. Cette technique permet une mise √† jour des param√®tres plus fr√©quente qu‚Äôavec une descente de gradient classique, un temps de calcul r√©duit et une convergence plus rapide, au d√©triment d‚Äôune approximation du gradient.\n",
    "\n",
    "Le code ci-dessous d√©crit le squelette de l‚Äôalgorithme de descente de gradient qui va permettre l‚Äôoptimisation des param√®tres du mod√®le :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95891c86-3b4f-4309-a99f-8300411865d1",
   "metadata": {},
   "source": [
    "Ce dessous quelques indices :\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Explication du code**\n",
    "Le programme entra√Æne un **mod√®le de classification lin√©aire** en utilisant **la descente de gradient par mini-batch**. Il apprend √† **pr√©dire des classes** √† partir de donn√©es d'entr√©e en utilisant une **fonction de perte cross-entropy** et une **fonction d'activation softmax**.\n",
    "\n",
    "### **1Ô∏è‚É£ Forward Pass (Pr√©diction)**\n",
    "On calcule les scores bruts des classes :\n",
    "$$\n",
    "\\text{logits} = XW + b\n",
    "$$\n",
    "Puis, on applique la **fonction softmax** pour obtenir des probabilit√©s.\n",
    "\n",
    "### **2Ô∏è‚É£ Calcul de la perte (Cross-Entropy)**\n",
    "- On transforme les √©tiquettes (`y_batch`) en **one-hot encoding**.\n",
    "- On utilise la **perte d'entropie crois√©e** :\n",
    "  $$\n",
    "  \\text{Perte} = -\\frac{1}{N} \\sum_{i} y_i \\log(\\hat{y}_i)\n",
    "  $$\n",
    "\n",
    "### **3Ô∏è‚É£ Backward Pass (Calcul des gradients)**\n",
    "- Le gradient de la perte par rapport aux logits est donn√© par :\n",
    "  $$\n",
    "  dL/d\\hat{Y} = \\hat{Y} - Y_{\\text{one-hot}}\n",
    "  $$\n",
    "- Ensuite, on calcule les gradients par rapport √† **W** et **b**.\n",
    "\n",
    "### **4Ô∏è‚É£ Mise √† jour des param√®tres (Descente de Gradient)**\n",
    "- On met √† jour `W` et `b` en soustrayant le gradient multipli√© par le **taux d'apprentissage (eta)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009d1ef-4634-4cfd-a5e5-e083586f7484",
   "metadata": {},
   "source": [
    "#### ** --> Formules des gradients de W et b**  \n",
    "\n",
    "##### **Gradient par rapport √† W (gradW)**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{1}{m} X^T (\\hat{Y} - Y)\n",
    "$$\n",
    "\n",
    "- $ X^T $ est la transpos√©e de la matrice des entr√©es.  \n",
    "- $ (\\hat{Y} - Y) $ repr√©sente la diff√©rence entre la pr√©diction et la vraie √©tiquette.\n",
    "\n",
    "##### **Gradient par rapport √† b (gradb)**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{Y}_i - Y_i)\n",
    "$$\n",
    "- Il s'agit simplement de la somme des erreurs sur chaque √©chantillon du batch.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå Mise √† jour des param√®tres**\n",
    "Une fois les gradients calcul√©s, on met √† jour les param√®tres via la **descente de gradient** :\n",
    "$$\n",
    "W = W - \\eta \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "$$\n",
    "b = b - \\eta \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "o√π $ \\eta $ est le **taux d'apprentissage** (learning rate).\n",
    "\n",
    "##### **‚úÖ R√©sum√©**\n",
    "| Gradient | Formule math√©matique |\n",
    "|----------|---------------------|\n",
    "| $ \\frac{\\partial L}{\\partial W} $ | $ \\frac{1}{m} X^T (\\hat{Y} - Y) $ |\n",
    "| $ \\frac{\\partial L}{\\partial b} $ | $ \\frac{1}{m} \\sum (\\hat{Y} - Y) $ |\n",
    "| Mise √† jour de $ W $ | $ W = W - \\eta \\cdot \\frac{\\partial L}{\\partial W} $ |\n",
    "| Mise √† jour de $ b $ | $ b = b - \\eta \\cdot \\frac{\\partial L}{\\partial b} $ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f27ead",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N, d = X_train.shape # N exemples, dimension d\n",
    "W = np.zeros((d, n_classes)) # initialisation de poids\n",
    "b = np.zeros((1, n_classes)) # initialisation des biais\n",
    "\n",
    "n_epochs = 20 # Nombre d'epochs de la descente de gradient\n",
    "eta = 1e-1 # Learning rate (pas d'apprentissage)\n",
    "batch_size = 100 # Taille du lot\n",
    "n_batches = int(float(N) / batch_size)\n",
    "\n",
    "# On alloue deux matrices pour stocker les valeurs des gradients\n",
    "gradW = np.zeros((d, n_classes))\n",
    "gradb = np.zeros((1, n_classes))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_idx in range(n_batches):\n",
    "        # ********* √Ä compl√©ter **********\n",
    "        # S√©lection du mini-batch\n",
    "        start = batch_idx * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = ...  # S√©lection des entr√©es du mini-batch\n",
    "        y_batch = ...  # S√©lection des labels du mini-batch\n",
    "\n",
    "        # ---- FORWARD PASS ----\n",
    "        logits = ...  # Calcul des scores bruts (logits)\n",
    "        softmax_probs = ...  # Conversion en probabilit√©s\n",
    "\n",
    "        # ---- CALCUL DE LA PERTE (CROSS-ENTROPY) ----\n",
    "        one_hot_y = ...  # Conversion des labels en one-hot encoding np.eye\n",
    "        y_log_y_pred = one_hot_y * np.log(softmax_probs + 1e-9) # Ajout d'un petit terme pour √©viter log(0)\n",
    "        loss =   ...\n",
    "\n",
    "        # ---- BACKWARD PASS ----\n",
    "        dL_dlogits = ...  # Gradient de la perte par rapport aux logits\n",
    "        gradW = ...  # Gradient par rapport √† W (en utilisant le np.dot() )\n",
    "        gradb = ...  # Gradient par rapport √† b\n",
    "        \n",
    "        # ---- MISE √Ä JOUR DES PARAM√àTRES ----\n",
    "        W -= ...\n",
    "        b -= ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb04541",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compl√©ter ce code. Vous devez notamment :\n",
    "\n",
    "> - √âcrire une fonction `forward(batch, W, b)` qui calcule la pr√©diction (vecteur de sortie $ \\hat{\\mathbf{y}} $ pour chaque exemple d‚Äôun batch de donn√©es. Si on consid√®re un batch des donn√©es de taille $ tb\\times 784 $, les param√®tres $ \\mathbf{W} $ (taille $ 784\\times 10 $) et $ \\mathbf{b} $ (taille $ 1\\times 10 $), la fonction `forward` renvoie la pr√©diction $ \\mathbf{\\hat{Y}} $ sur le batch (taille $ tb\\times 10 $).  La fonction `forward` sera appel√©e pour chaque it√©ration de la double boucle pr√©c√©dente.  \n",
    "- Completer la fonction `softmax` ci-dessous pour calculer le r√©sultat du passage du softmax sur chaque √©l√©ment de de la matrice de la projection lin√©raire (taille $ tb\\times 10 $) :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5b373-b44e-4dc9-80e2-f397a12afebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X_batch, W, b ):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ef32d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "     # Entr√©e: matrice X de dimensions batch x d\n",
    "     # Sortie: matrice de m√™mes dimensions\n",
    "     ...\n",
    "\n",
    "    return softmax_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa6c02",
   "metadata": {},
   "source": [
    "\n",
    "- \n",
    "  <dl style='margin: 20px 0;'>\n",
    "  <dt>r√©ecrire le code d'avant avec les deux nouvelles fonctions</dt>\n",
    "  \n",
    "  </dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cd5e6-cc07-4217-bb80-b68e48a080ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccef8bff",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "√âvaluer les performances du mod√®le de r√©gression logistique entra√Æn√© sur MNIST. On utilisera le taux de bonne classification (*accuracy*) comme m√©trique. Commencer par mesurer l‚Äô√©volution des performances du mod√®le au cours de l‚Äôapprentissage (calcul de l'*accuracy* √† chaque √©poque), puis √©valuer sur le mod√®le sur la base de test. Vous pouvez utiliser la [fonction de scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) ou la fonction `accuracy` ci-dessous (qui effectue √©galement la phase de pr√©diction).\n",
    "\n",
    "**Vous devriez obtenir un score de l‚Äôordre de 92% sur la base de test pour ce mod√®le de r√©gression logistique.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5de83",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def accuracy(W, b, images, labels):\n",
    "    \"\"\" W: matrice de param√®tres\n",
    "        b: vecteur de biais\n",
    "        images: images de MNIST\n",
    "        labels: √©tiquettes de MNIST pour les images\n",
    "\n",
    "        Renvoie l'accuracy du mod√®le (W, b) sur les images par rapport aux labels\n",
    "    \"\"\"\n",
    "    pred = forward(images, W, b)\n",
    "    return np.where(pred.argmax(axis=1) != labels.argmax(axis=1), 0.,1.).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2abc1be-8b96-4888-ad03-8598883b8b07",
   "metadata": {},
   "source": [
    "##### Utilise le package sklearn pour entra√Æner un MLP (avec la m√™me architecture que le r√©seau pr√©c√©dent) ainsi qu‚Äôun SVM. √âvalue ensuite les deux mod√®les et compare les r√©sultats obtenus avec ceux du mod√®le pr√©c√©dent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f440de9-b250-4321-8ebe-592c12beceab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "date": 1725613532.7446063,
  "filename": "tpDeepLearning1.rst",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "title": "Travaux pratiques - Premiers r√©seaux de neurones"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
